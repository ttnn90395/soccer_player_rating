{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d811d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium tqdm bs4 playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7746ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e071a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BASE_URL = (\n",
    "    \"https://www.footballtransfers.com/en/values/players/most-valuable-players/{}\"\n",
    ")\n",
    "MAX_RETRIES = 8\n",
    "THREADS = 7\n",
    "MAX_PAGES = 1353\n",
    "TIMEOUT_SECONDS = 1\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "DATA_FILE = \"most_valuable_players_fast.csv\"\n",
    "SCRAPED_PAGES_LOG = \"scraped_pages.txt\"\n",
    "FAILED_PAGES_LOG = \"failed_pages.txt\"\n",
    "\n",
    "\n",
    "# Page Logging Helpers\n",
    "def load_page_log(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return set(int(line.strip()) for line in f if line.strip().isdigit())\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "\n",
    "def log_page(file_path, page_number):\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"{page_number}\\n\")\n",
    "\n",
    "\n",
    "# Setup WebDriver\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.binary_location = CHROMIUM_PATH\n",
    "    options.add_argument(\"--headless=chrome\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    prefs = {\n",
    "        \"profile.managed_default_content_settings.images\": 2,\n",
    "        \"profile.managed_default_content_settings.stylesheets\": 2,\n",
    "        \"profile.managed_default_content_settings.cookies\": 2,\n",
    "        \"profile.managed_default_content_settings.javascript\": 1,\n",
    "        \"profile.managed_default_content_settings.plugins\": 2,\n",
    "        \"profile.managed_default_content_settings.popups\": 2,\n",
    "        \"profile.managed_default_content_settings.geolocation\": 2,\n",
    "        \"profile.managed_default_content_settings.media_stream\": 2,\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "# Parse HTML Page\n",
    "def parse_html(page_source):\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    rows = soup.find_all(\"tr\")\n",
    "    data = []\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            name_tag = row.select_one(\"td.td-player a[title]\")\n",
    "            if not name_tag:\n",
    "                continue\n",
    "            name = name_tag.get_text(strip=True)\n",
    "            player_url = \"https://www.footballtransfers.com\" + name_tag[\"href\"]\n",
    "\n",
    "            age_tag = row.select_one(\"td.age\")\n",
    "            age = age_tag.get_text(strip=True) if age_tag else None\n",
    "\n",
    "            club_tag = row.select_one(\"td.td-player .sub-text a[title]\")\n",
    "            club = club_tag.get_text(strip=True) if club_tag else None\n",
    "            club_url = (\n",
    "                \"https://www.footballtransfers.com\" + club_tag[\"href\"]\n",
    "                if club_tag\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            position_tag = row.select_one(\"td.td-player span.sub-text\")\n",
    "            position = (\n",
    "                position_tag.get_text(strip=True).split(\"‚Ä¢\")[-1].strip()\n",
    "                if position_tag\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            nationality_tag = row.select_one(\"td.td-player figure img\")\n",
    "            nationality = nationality_tag[\"alt\"] if nationality_tag else None\n",
    "\n",
    "            skill_tag = row.select_one(\"div.table-skill__skill\")\n",
    "            skill = float(skill_tag.get_text(strip=True)) if skill_tag else None\n",
    "\n",
    "            potential_tag = row.select_one(\"div.table-skill__pot\")\n",
    "            potential = (\n",
    "                float(potential_tag.get_text(strip=True)) if potential_tag else None\n",
    "            )\n",
    "\n",
    "            value_tag = row.select_one(\"span.player-tag\")\n",
    "            value = (\n",
    "                value_tag.get_text(strip=True).replace(\"‚Ç¨\", \"\") if value_tag else None\n",
    "            )\n",
    "\n",
    "            if value and \"M\" in value:\n",
    "                market_value = float(value.replace(\"M\", \"\")) * 1e6\n",
    "            elif value and \"K\" in value:\n",
    "                market_value = float(value.replace(\"K\", \"\")) * 1e3\n",
    "            else:\n",
    "                market_value = None\n",
    "\n",
    "            data.append(\n",
    "                {\n",
    "                    \"Name\": name,\n",
    "                    \"Player URL\": player_url,\n",
    "                    \"Age\": age,\n",
    "                    \"Club\": club,\n",
    "                    \"Club URL\": club_url,\n",
    "                    \"Position\": position,\n",
    "                    \"Nationality\": nationality,\n",
    "                    \"Skill\": skill,\n",
    "                    \"Potential\": potential,\n",
    "                    \"Market Value (‚Ç¨)\": market_value,\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Parse error: {e}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# Scrape Range\n",
    "def scrape_page_range(start_page, end_page, scraped_pages):\n",
    "    driver = create_driver()\n",
    "    wait = WebDriverWait(driver, TIMEOUT_SECONDS)\n",
    "\n",
    "    for page in tqdm(\n",
    "        range(start_page, end_page + 1),\n",
    "        desc=f\"Thread {start_page}-{end_page}\",\n",
    "        leave=False,\n",
    "    ):\n",
    "        if page in scraped_pages:\n",
    "            print(f\"Skipping page {page} (already scraped)\")\n",
    "            continue\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                url = BASE_URL.format(page)\n",
    "                driver.get(url)\n",
    "                wait.until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (By.CSS_SELECTOR, \"td.td-player a[title]\")\n",
    "                    )\n",
    "                )\n",
    "                data = parse_html(driver.page_source)\n",
    "\n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df.to_csv(\n",
    "                        DATA_FILE,\n",
    "                        mode=\"a\",\n",
    "                        header=not os.path.exists(DATA_FILE),\n",
    "                        index=False,\n",
    "                    )\n",
    "                    log_page(SCRAPED_PAGES_LOG, page)\n",
    "                    print(f\"Saved page {page} with {len(data)} players\")\n",
    "                else:\n",
    "                    print(f\"No data found on page {page}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Retry {attempt} failed on page {page}: {e}\")\n",
    "                time.sleep(2 * attempt)\n",
    "                if attempt == MAX_RETRIES:\n",
    "                    log_page(FAILED_PAGES_LOG, page)\n",
    "                    print(f\"Failed page {page} after {MAX_RETRIES} retries\")\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    chunk_size = math.ceil(MAX_PAGES / THREADS)\n",
    "    ranges = [\n",
    "        (i, min(i + chunk_size - 1, MAX_PAGES))\n",
    "        for i in range(1, MAX_PAGES + 1, chunk_size)\n",
    "    ]\n",
    "    scraped_pages = load_page_log(SCRAPED_PAGES_LOG)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        futures = [\n",
    "            executor.submit(scrape_page_range, start, end, scraped_pages)\n",
    "            for start, end in ranges\n",
    "        ]\n",
    "        for future in tqdm(\n",
    "            as_completed(futures), total=len(futures), desc=\"Scraping Progress\"\n",
    "        ):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Thread error: {e}\")\n",
    "\n",
    "    # Summary Report\n",
    "    scraped = load_page_log(SCRAPED_PAGES_LOG)\n",
    "    failed = load_page_log(FAILED_PAGES_LOG)\n",
    "    all_attempted = scraped | failed\n",
    "    skipped = set(range(1, MAX_PAGES + 1)) - all_attempted\n",
    "\n",
    "    print(\"Summary:\")\n",
    "    print(f\"Scraped pages: {len(scraped)}\")\n",
    "    print(f\"Failed pages: {len(failed)} (see '{FAILED_PAGES_LOG}')\")\n",
    "    print(f\"Skipped pages: {len(skipped)} (not attempted yet)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "# Read file as raw text\n",
    "with open(\"most_valuable_players_fast.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    csv_data = f.read()\n",
    "\n",
    "# Replace doubled double-quotes (\"\"M, AM (R)\"\") with proper quoting (\"M, AM (R)\")\n",
    "csv_data = csv_data.replace('\"\"', '\"')\n",
    "\n",
    "# Load CSV from string safely\n",
    "df = pd.read_csv(StringIO(csv_data), quotechar='\"', skipinitialspace=True)\n",
    "\n",
    "# Convert numeric columns\n",
    "for col in [\"Age\", \"Rating\", \"Potential\", \"Value\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Show preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Your raw CSV string\n",
    "csv_data = \"\"\"Name,Player link,Age,Team,Team Link,Positions,Nationality,Rating,Potential,Value\n",
    "Erling Haaland,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/erling-braut-haaland,24,Man City,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/uk/man-city,F (C),Norway,91.7,99.3,184200000.0\n",
    "Lamine Yamal,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/lamine-yamal,17,Barcelona,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/es/barcelona,\"M, AM (R)\",Spain,83.9,100.0,183200000.0\n",
    "Kylian Mbapp√©,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/kylian-mbappe,26,Real Madrid,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/es/real-madrid,F (C),France,89.7,93.8,153200000.0\n",
    "Pedri,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/pedri-1,22,Barcelona,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/es/barcelona,\"M, DM, AM (C)\",Spain,83.8,97.5,137900000.0\n",
    "Jude Bellingham,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/jude-bellingham,22,Real Madrid,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/es/real-madrid,\"AM (C), M (CL)\",England,83.1,97.7,137800000.0\n",
    "Florian Wirtz,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/florian-wirtz,22,Liverpool,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/uk/liverpool,AM (C),Germany,86.3,98.8,135000000.0\n",
    "Pau Cubars√≠,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/pau-cubarsi,18,Barcelona,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/es/barcelona,D (C),Spain,71.7,100.0,130900000.0\n",
    "Jamal Musiala,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/jamal-musiala,22,Bayern,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/de/bayern,M (C),Germany,89.4,100.0,129300000.00000001\n",
    "\"\"\"\n",
    "\n",
    "# Parse the CSV string safely\n",
    "df = pd.read_csv(StringIO(csv_data), quotechar='\"')\n",
    "\n",
    "# Optional: convert numeric fields\n",
    "for col in [\"Age\", \"Rating\", \"Potential\", \"Value\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Preview clean data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"most_valuable_players_fast.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    csv_data = f.read()\n",
    "\n",
    "print(csv_data[:1000])  # Preview first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9074a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fixed_rows = []\n",
    "expected_columns = 10\n",
    "\n",
    "with open(\"most_valuable_players_fast.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "\n",
    "        # If entire line is wrapped in quotes, unwrap it first\n",
    "        if line.startswith('\"') and line.endswith('\"'):\n",
    "            line = line[1:-1]\n",
    "\n",
    "        # Fix escaped quotes like \"\"M, AM (R)\"\" to \"M, AM (R)\"\n",
    "        line = line.replace('\"\"', '\"')\n",
    "\n",
    "        # Parse line using csv.reader for proper quote handling\n",
    "        parsed = next(csv.reader([line], quotechar='\"', skipinitialspace=True))\n",
    "\n",
    "        # If the row looks good, keep it\n",
    "        if len(parsed) == expected_columns:\n",
    "            fixed_rows.append(parsed)\n",
    "        elif len(parsed) > expected_columns:\n",
    "            # Try merging overflow into Positions field\n",
    "            repaired = parsed[:5] + [\", \".join(parsed[5:-4])] + parsed[-4:]\n",
    "            if len(repaired) == expected_columns:\n",
    "                fixed_rows.append(repaired)\n",
    "            else:\n",
    "                print(f\"Could not fix row: {parsed}\")\n",
    "\n",
    "# Load into DataFrame\n",
    "columns = [\n",
    "    \"Name\",\n",
    "    \"Player URL\",\n",
    "    \"Age\",\n",
    "    \"Team\",\n",
    "    \"Team Link\",\n",
    "    \"Positions\",\n",
    "    \"Nationality\",\n",
    "    \"Rating\",\n",
    "    \"Potential\",\n",
    "    \"Value\",\n",
    "]\n",
    "df = pd.DataFrame(fixed_rows, columns=columns)\n",
    "\n",
    "# Convert numeric columns\n",
    "for col in [\"Age\", \"Rating\", \"Potential\", \"Value\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Preview cleaned data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(index=0).reset_index(drop=True)\n",
    "# Define the redundant prefix\n",
    "prefix = \"https://www.footballtransfers.com\"\n",
    "\n",
    "# Fix repeated prefix in all relevant URL columns\n",
    "for col in [\"Player URL\", \"Team Link\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].str.replace(f\"{prefix}{prefix}\", prefix, regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8746355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all_players_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'Value' descending\n",
    "df_sorted = df.sort_values(by=\"Value\", ascending=False)\n",
    "\n",
    "# Preview top 10 most valuable players\n",
    "df_sorted[[\"Name\", \"Team\", \"Value\"]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for French players\n",
    "french_players = df[df[\"Nationality\"] == \"France\"]\n",
    "\n",
    "# Optional: sort by Value descending\n",
    "french_players_sorted = french_players.sort_values(by=\"Rating\", ascending=False)\n",
    "\n",
    "# Show relevant columns\n",
    "french_players_sorted[[\"Name\", \"Team\", \"Value\", \"Rating\", \"Potential\"]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43dd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee84807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "\n",
    "def run_scrape():\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "        page.goto(\n",
    "            \"https://fbref.com/en/players/7a2e46a8/all_comps/Alisson-Stats---All-Competitions\"\n",
    "        )\n",
    "        page.wait_for_selector(\"#stats_keeper_expanded\")\n",
    "        table_html = page.locator(\"#stats_keeper_expanded\").inner_html()\n",
    "        print(table_html)\n",
    "        browser.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cfb988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "\n",
    "\n",
    "async def fetch_table_and_save_csv():\n",
    "    url = \"https://fbref.com/en/players/7a2e46a8/all_comps/Alisson-Stats---All-Competitions\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # Wait for rows to ensure content is loaded\n",
    "        await page.wait_for_selector(\"#stats_keeper_expanded >> tbody >> tr\")\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"id\": \"stats_keeper_expanded\"})\n",
    "    if not table:\n",
    "        print(\"Table not found.\")\n",
    "        return\n",
    "\n",
    "    # Use last <tr> in <thead> as the real header row\n",
    "    header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "\n",
    "    data = []\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        # Include potential <th> (for sticky first column like \"Season\")\n",
    "        ths = [th.get_text(strip=True) for th in tr.find_all(\"th\")]\n",
    "        tds = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "        row = ths + tds  # combine <th> and <td> values\n",
    "\n",
    "        if len(row) == len(headers):\n",
    "            data.append(row)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Row mismatch: expected {len(headers)} but got {len(row)} ‚Üí Skipped row:\",\n",
    "                row,\n",
    "            )\n",
    "\n",
    "    if not data:\n",
    "        print(\"No valid data rows found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    df.to_csv(\"alisson_goalkeeping_stats.csv\", index=False)\n",
    "    print(\"Saved as alisson_goalkeeping_stats.csv\")\n",
    "\n",
    "\n",
    "# Run it\n",
    "asyncio.run(fetch_table_and_save_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "BASE_URL = \"https://fbref.com\"\n",
    "\n",
    "\n",
    "async def fetch_table_and_save_csv():\n",
    "    url = f\"{BASE_URL}/en/players/7a2e46a8/all_comps/Alisson-Stats---All-Competitions\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # Wait for actual data rows to be visible\n",
    "        await page.wait_for_selector(\"#stats_keeper_expanded >> tbody >> tr\")\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"id\": \"stats_keeper_expanded\"})\n",
    "    if not table:\n",
    "        print(\"Table not found.\")\n",
    "        return\n",
    "\n",
    "    # Get accurate column names from last header row\n",
    "    header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "\n",
    "    data = []\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        row = []\n",
    "\n",
    "        # Handle sticky <th> cells (e.g. \"Season\") and add link if present\n",
    "        for th in tr.find_all(\"th\"):\n",
    "            text = th.get_text(strip=True)\n",
    "            link = th.find(\"a\")\n",
    "            if link and link.get(\"href\"):\n",
    "                text += f\" ({BASE_URL}{link.get('href')})\"\n",
    "            row.append(text)\n",
    "\n",
    "        # Handle <td> cells with potential links\n",
    "        for td in tr.find_all(\"td\"):\n",
    "            text = td.get_text(strip=True)\n",
    "            link = td.find(\"a\")\n",
    "            if link and link.get(\"href\"):\n",
    "                text += f\" ({BASE_URL}{link.get('href')})\"\n",
    "            row.append(text)\n",
    "\n",
    "        if len(row) == len(headers):\n",
    "            data.append(row)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Row mismatch: expected {len(headers)} but got {len(row)} ‚Üí Skipping.\"\n",
    "            )\n",
    "\n",
    "    if not data:\n",
    "        print(\"No valid data rows found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    df.to_csv(\"alisson_goalkeeping_stats_upgraded.csv\", index=False)\n",
    "    print(\"Saved as alisson_goalkeeping_stats_upgraded.csv with links included\")\n",
    "\n",
    "\n",
    "# Execute the async function\n",
    "asyncio.run(fetch_table_and_save_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# Patch event loop (required for Jupyter or interactive mode)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://fbref.com\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "\n",
    "\n",
    "async def fetch_national_team_stats():\n",
    "    url = f\"{BASE_URL}/en/players/7a2e46a8/all_comps/Alisson-Stats---All-Competitions\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # OPTIONAL: Wait for a key element (e.g. \"Matches\" tab) to ensure page loads\n",
    "        await asyncio.sleep(4)  # allow time for JS-rendered tables\n",
    "\n",
    "        # Get page HTML\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Locate the correct national team keeper table by ID\n",
    "    table = soup.find(\"table\", {\"id\": \"stats_keeper_nat_tm\"})\n",
    "    if not table:\n",
    "        print(\"National Team goalkeeping table not found.\")\n",
    "        return\n",
    "\n",
    "    # Extract column headers\n",
    "    header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "\n",
    "    # Extract table rows\n",
    "    data = []\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        row = []\n",
    "\n",
    "        # <th> cells (e.g. season) + optional link\n",
    "        for th in tr.find_all(\"th\"):\n",
    "            text = th.get_text(strip=True)\n",
    "            link = th.find(\"a\")\n",
    "            if link and link.get(\"href\"):\n",
    "                text += f\" ({BASE_URL}{link.get('href')})\"\n",
    "            row.append(text)\n",
    "\n",
    "        # <td> cells + optional link\n",
    "        for td in tr.find_all(\"td\"):\n",
    "            text = td.get_text(strip=True)\n",
    "            link = td.find(\"a\")\n",
    "            if link and link.get(\"href\"):\n",
    "                text += f\" ({BASE_URL}{link.get('href')})\"\n",
    "            row.append(text)\n",
    "\n",
    "        # Verify row matches header length\n",
    "        if len(row) == len(headers):\n",
    "            data.append(row)\n",
    "        else:\n",
    "            print(f\"Skipped row ({len(row)} vs {len(headers)} columns):\", row)\n",
    "\n",
    "    # Export to CSV\n",
    "    if data:\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        df.to_csv(\"alisson_national_team_stats.csv\", index=False)\n",
    "        print(\"Saved as alisson_national_team_stats.csv with embedded links\")\n",
    "    else:\n",
    "        print(\"No valid rows found.\")\n",
    "\n",
    "\n",
    "# Run the function in Notebook\n",
    "await fetch_national_team_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Settings\n",
    "BASE_URL = \"https://fbref.com\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "SAVE_FOLDER = \"alisson_all_fbref_tables\"\n",
    "\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "async def scrape_all_fbref_tables():\n",
    "    url = f\"{BASE_URL}/en/players/7a2e46a8/all_comps/Alisson-Stats---All-Competitions\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # Trigger all preset buttons to reveal toggled tables\n",
    "        await page.evaluate(\"\"\"\n",
    "            document.querySelectorAll('a.sr_preset').forEach(el => el.click());\n",
    "        \"\"\")\n",
    "        await asyncio.sleep(5)  # Let the page render updated sections\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tables = soup.find_all(\"table\")\n",
    "\n",
    "    print(f\"üîç Found {len(tables)} tables on page\")\n",
    "\n",
    "    for table in tables:\n",
    "        table_id = table.get(\"id\", None)\n",
    "        if not table_id:\n",
    "            continue  # skip tables without ID\n",
    "\n",
    "        print(f\"Processing table: {table_id}\")\n",
    "\n",
    "        # Extract headers\n",
    "        header_row = (\n",
    "            table.find(\"thead\").find_all(\"tr\")[-1] if table.find(\"thead\") else None\n",
    "        )\n",
    "        headers = (\n",
    "            [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "            if header_row\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        rows = []\n",
    "        for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            row = []\n",
    "            for cell in tr.find_all([\"th\", \"td\"]):\n",
    "                text = cell.get_text(strip=True)\n",
    "                link = cell.find(\"a\")\n",
    "                if link and link.get(\"href\"):\n",
    "                    text += f\" ({BASE_URL}{link.get('href')})\"\n",
    "                row.append(text)\n",
    "            if len(row) == len(headers):\n",
    "                rows.append(row)\n",
    "\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows, columns=headers)\n",
    "            output_path = os.path.join(SAVE_FOLDER, f\"{table_id}.csv\")\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved: {output_path}\")\n",
    "        else:\n",
    "            print(f\"Skipped empty table: {table_id}\")\n",
    "\n",
    "\n",
    "# Run the function\n",
    "await scrape_all_fbref_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual file path or name\n",
    "df = pd.read_csv(\"all_players_ratings.csv\")\n",
    "\n",
    "# Optional: Preview the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "CSV_PATH = \"all_players_ratings.csv\"  # Update with your actual file\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "SEARCH_BASE = \"https://fbref.com/en/search/search.fcgi?search=\"\n",
    "\n",
    "# Load player list\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = None  # New column to store FBref profile link\n",
    "\n",
    "\n",
    "async def search_fbref_for_players():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            name = str(row[\"Name\"]).strip()\n",
    "            search_url = SEARCH_BASE + name.replace(\" \", \"+\")\n",
    "            await page.goto(search_url)\n",
    "            await page.wait_for_timeout(1500)  # small delay to allow page content\n",
    "\n",
    "            # Look for first profile link\n",
    "            links = await page.locator(\"a\").all()\n",
    "            match = None\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"/en/players/\" in href and \"all_comps\" in href:\n",
    "                    match = \"https://fbref.com\" + href\n",
    "                    break\n",
    "\n",
    "            if match:\n",
    "                df.at[i, \"fbref_url\"] = match\n",
    "                print(f\"Found: {name} ‚Üí {match}\")\n",
    "            else:\n",
    "                print(f\"Not found: {name}\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # Export enriched data\n",
    "    output_path = \"players_with_fbref_links.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"All done! Saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Run the function\n",
    "await search_fbref_for_players()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "SEARCH_BASE = \"https://fbref.com/en/search/search.fcgi?search=\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = None\n",
    "\n",
    "\n",
    "async def search_fbref_for_players():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            name = str(row[\"Name\"]).strip()\n",
    "            search_url = SEARCH_BASE + name.replace(\" \", \"+\")\n",
    "            await page.goto(search_url)\n",
    "            await page.wait_for_timeout(1500)\n",
    "\n",
    "            links = await page.locator(\"a\").all()\n",
    "            match_url = None\n",
    "\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"/en/players/\" in href:\n",
    "                    try:\n",
    "                        after_players = href.split(\"/players/\")[1]\n",
    "                        player_id = after_players.split(\"/\")[0]\n",
    "                        name_slug = name.replace(\" \", \"-\") + \"-Stats---All-Competitions\"\n",
    "                        match_url = f\"https://fbref.com/en/players/{player_id}/all_comps/{name_slug}\"\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "            if match_url:\n",
    "                df.at[i, \"fbref_url\"] = match_url\n",
    "                print(f\"{name} ‚Üí {match_url}\")\n",
    "            else:\n",
    "                print(f\"Not found: {name}\")\n",
    "\n",
    "        await browser.close()\n",
    "        df.to_csv(\"players_with_fbref_links.csv\", index=False)\n",
    "        print(\"Saved as players_with_fbref_links.csv\")\n",
    "\n",
    "\n",
    "await search_fbref_for_players()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bfda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"  # Update with your actual file path\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "DUCKDUCKGO_SEARCH = \"https://duckduckgo.com/?q=site%3Afbref.com+\"\n",
    "\n",
    "# Load players from CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = None  # Add column for results\n",
    "\n",
    "\n",
    "async def search_fbref_via_duckduckgo():\n",
    "    async with async_playwright() as p:\n",
    "        # Launch browser\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        # Loop through players\n",
    "        for i, row in df.iterrows():\n",
    "            name = str(row[\"Name\"]).strip()\n",
    "            query_url = DUCKDUCKGO_SEARCH + name.replace(\" \", \"+\")\n",
    "            await page.goto(query_url)\n",
    "            await page.wait_for_timeout(2000)  # Wait for search results to load\n",
    "\n",
    "            match = None\n",
    "            links = await page.locator(\"a\").all()\n",
    "\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"fbref.com/en/players/\" in href:\n",
    "                    match = href\n",
    "                    break\n",
    "\n",
    "            if match:\n",
    "                df.at[i, \"fbref_url\"] = match\n",
    "                print(f\"Found: {name} ‚Üí {match}\")\n",
    "            else:\n",
    "                print(f\"Not found: {name}\")\n",
    "\n",
    "        # Clean up\n",
    "        await browser.close()\n",
    "\n",
    "    # Save results\n",
    "    output_path = \"players_with_fbref_links_duckduckgo.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"All done! Saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Run the scraping function\n",
    "await search_fbref_via_duckduckgo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "DUCKDUCKGO_SEARCH = \"https://duckduckgo.com/?q=site%3Afbref.com+\"\n",
    "LOG_PATH = \"scraping_log.txt\"\n",
    "NUM_TABS = 5\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = df.get(\"fbref_url\", None)\n",
    "\n",
    "with open(LOG_PATH, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(f\"Scraping started at {datetime.now()}\\n\\n\")\n",
    "\n",
    "\n",
    "# Scraping logic with retries\n",
    "async def scrape_player(tab, index, name):\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            query_url = DUCKDUCKGO_SEARCH + name.replace(\" \", \"+\")\n",
    "            await tab.goto(query_url)\n",
    "            await tab.wait_for_timeout(1500)\n",
    "\n",
    "            links = await tab.locator(\"a\").all()\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"fbref.com/en/players/\" in href:\n",
    "                    df.at[index, \"fbref_url\"] = href\n",
    "                    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                        log.write(f\"Found (Attempt {attempt}): {name} ‚Üí {href}\\n\")\n",
    "                    return\n",
    "            # If we get here, no match found\n",
    "        except Exception as e:\n",
    "            with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\" Error on {name} (Attempt {attempt}): {e}\\n\")\n",
    "\n",
    "    # Final failure\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"Not found after {MAX_RETRIES} attempts: {name}\\n\")\n",
    "\n",
    "\n",
    "async def scrape_batch(tab, batch):\n",
    "    for index, name in tqdm(\n",
    "        batch, desc=f\"Tab scraping {len(batch)} players\", leave=False\n",
    "    ):\n",
    "        await scrape_player(tab, index, name)\n",
    "\n",
    "\n",
    "async def search_fbref_multi_tab():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        tabs = [await browser.new_page() for _ in range(NUM_TABS)]\n",
    "\n",
    "        all_players = [(i, str(row[\"Name\"]).strip()) for i, row in df.iterrows()]\n",
    "        batch_size = math.ceil(len(all_players) / NUM_TABS)\n",
    "        batches = [\n",
    "            all_players[i : i + batch_size]\n",
    "            for i in range(0, len(all_players), batch_size)\n",
    "        ]\n",
    "\n",
    "        await asyncio.gather(\n",
    "            *[scrape_batch(tab, batch) for tab, batch in zip(tabs, batches)]\n",
    "        )\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(\"Scraping finished with retries and progress bar. CSV and log updated.\")\n",
    "\n",
    "\n",
    "await search_fbref_multi_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "FBREF_SEARCH = \"https://fbref.com/en/search/search.fcgi?search=\"\n",
    "LOG_PATH = \"scraping_log.txt\"\n",
    "NUM_TABS = 1\n",
    "MAX_RETRIES = 3\n",
    "REQUEST_THROTTLE_SEC = 2\n",
    "STARTUP_DELAY_SEC = 0\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = df.get(\"fbref_url\", None)\n",
    "\n",
    "# Init log file\n",
    "with open(LOG_PATH, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(f\"Scraping initialized at {datetime.now()}\\n\\n\")\n",
    "\n",
    "\n",
    "# Player scraping logic\n",
    "async def scrape_player(tab, index, name):\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            search_url = FBREF_SEARCH + name.replace(\" \", \"+\")\n",
    "            await tab.goto(search_url)\n",
    "            await tab.wait_for_timeout(1500)\n",
    "            await asyncio.sleep(REQUEST_THROTTLE_SEC)\n",
    "\n",
    "            match = None\n",
    "            links = await tab.locator(\"a\").all()\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"/en/players/\" in href:\n",
    "                    match = \"https://fbref.com\" + href\n",
    "                    break\n",
    "\n",
    "            if not match:\n",
    "                table_links = await tab.locator(\"table a\").all()\n",
    "                for link in table_links:\n",
    "                    href = await link.get_attribute(\"href\")\n",
    "                    if href and \"/en/players/\" in href:\n",
    "                        match = \"https://fbref.com\" + href\n",
    "                        break\n",
    "\n",
    "            with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                if match:\n",
    "                    df.at[index, \"fbref_url\"] = match\n",
    "                    log.write(f\"Found (Attempt {attempt}): {name} ‚Üí {match}\\n\")\n",
    "                    return\n",
    "                else:\n",
    "                    log.write(f\"No match on attempt {attempt}: {name}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"Error on {name} (Attempt {attempt}): {e}\\n\")\n",
    "\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"Not found after {MAX_RETRIES} attempts: {name}\\n\")\n",
    "\n",
    "\n",
    "# üßπ Batch logic\n",
    "async def scrape_batch(tab, batch):\n",
    "    for index, name in tqdm(\n",
    "        batch, desc=f\" Tab scraping {len(batch)} players\", leave=False\n",
    "    ):\n",
    "        await scrape_player(tab, index, name)\n",
    "\n",
    "\n",
    "# Orchestration\n",
    "async def search_fbref_multi_tab():\n",
    "    print(\"Waiting 1 hour before scraping...\")\n",
    "    await asyncio.sleep(STARTUP_DELAY_SEC)\n",
    "    print(\"Starting scraping...\")\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        tabs = [await browser.new_page() for _ in range(NUM_TABS)]\n",
    "\n",
    "        all_players = [(i, str(row[\"Name\"]).strip()) for i, row in df.iterrows()]\n",
    "        batch_size = math.ceil(len(all_players) / NUM_TABS)\n",
    "        batches = [\n",
    "            all_players[i : i + batch_size]\n",
    "            for i in range(0, len(all_players), batch_size)\n",
    "        ]\n",
    "\n",
    "        await asyncio.gather(\n",
    "            *[scrape_batch(tab, batch) for tab, batch in zip(tabs, batches)]\n",
    "        )\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Scraping finished. Updated CSV and log saved to {LOG_PATH}\")\n",
    "\n",
    "\n",
    "# Run it\n",
    "await search_fbref_multi_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "GOOGLE_SEARCH = \"https://www.google.com/search?q=site%3Afbref.com+\"\n",
    "LOG_PATH = \"scraping_log.txt\"\n",
    "NUM_TABS = 1\n",
    "MAX_RETRIES = 3\n",
    "REQUESTS_PER_MIN = 30\n",
    "STARTUP_DELAY_SEC = 0\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = df.get(\"fbref_url\", None)\n",
    "\n",
    "# Initialize log file\n",
    "with open(LOG_PATH, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(f\"Scraping initialized at {datetime.now()}\\n\\n\")\n",
    "\n",
    "# Rate limiter across tabs\n",
    "rate_lock = asyncio.Lock()\n",
    "RATE_INTERVAL = 60 / REQUESTS_PER_MIN  # seconds between each request\n",
    "\n",
    "\n",
    "# Scraping logic with shared global throttle\n",
    "async def scrape_player(tab, index, name, club):\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            async with rate_lock:\n",
    "                await asyncio.sleep(RATE_INTERVAL)  # enforce request interval globally\n",
    "\n",
    "            query = f\"{name} {club}\".replace(\" \", \"+\")\n",
    "            search_url = GOOGLE_SEARCH + query\n",
    "            await tab.goto(search_url)\n",
    "            await tab.wait_for_timeout(2000)\n",
    "\n",
    "            match = None\n",
    "            links = await tab.locator(\"a\").all()\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"fbref.com/en/players/\" in href:\n",
    "                    match = href\n",
    "                    break\n",
    "\n",
    "            with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                if match:\n",
    "                    df.at[index, \"fbref_url\"] = match\n",
    "                    log.write(f\"Found (Attempt {attempt}): {name} ({club}) ‚Üí {match}\\n\")\n",
    "                    return\n",
    "                else:\n",
    "                    log.write(f\"üîç No match on attempt {attempt}: {name} ({club})\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"Error on {name} ({club}) (Attempt {attempt}): {e}\\n\")\n",
    "\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"Not found after {MAX_RETRIES} attempts: {name} ({club})\\n\")\n",
    "\n",
    "\n",
    "# Batch logic\n",
    "async def scrape_batch(tab, batch):\n",
    "    for index, name, club in tqdm(\n",
    "        batch, desc=f\"Scraping {len(batch)} players\", leave=False\n",
    "    ):\n",
    "        await scrape_player(tab, index, name, club)\n",
    "\n",
    "\n",
    "# Orchestration\n",
    "async def search_fbref_multi_tab():\n",
    "    if STARTUP_DELAY_SEC:\n",
    "        print(f\"Waiting {STARTUP_DELAY_SEC} seconds before scraping...\")\n",
    "        await asyncio.sleep(STARTUP_DELAY_SEC)\n",
    "    print(\"Starting scraping...\")\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        tabs = [await browser.new_page() for _ in range(NUM_TABS)]\n",
    "\n",
    "        all_players = [\n",
    "            (i, str(row[\"Name\"]).strip(), str(row.get(\"Club\", \"\")).strip())\n",
    "            for i, row in df.iterrows()\n",
    "        ]\n",
    "        batch_size = math.ceil(len(all_players) / NUM_TABS)\n",
    "        batches = [\n",
    "            all_players[i : i + batch_size]\n",
    "            for i in range(0, len(all_players), batch_size)\n",
    "        ]\n",
    "\n",
    "        await asyncio.gather(\n",
    "            *[scrape_batch(tab, batch) for tab, batch in zip(tabs, batches)]\n",
    "        )\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Done! Updated CSV and saved logs to {LOG_PATH}\")\n",
    "\n",
    "\n",
    "# Launch\n",
    "await search_fbref_multi_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99659121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "DUCKDUCKGO_SEARCH = \"https://duckduckgo.com/?q=site%3Afbref.com+\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "LOG_PATH = \"scraping_log.txt\"\n",
    "RATE_LIMIT = 60 * 20  # requests per minute\n",
    "RATE_INTERVAL = 60 / RATE_LIMIT  # seconds between requests\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"fbref_url\" not in df.columns:\n",
    "    df[\"fbref_url\"] = None\n",
    "\n",
    "# Init log\n",
    "with open(LOG_PATH, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(f\"Scraping started at {datetime.now()}\\n\\n\")\n",
    "\n",
    "\n",
    "# Single player search\n",
    "async def scrape_player(tab, index, name, club):\n",
    "    await asyncio.sleep(RATE_INTERVAL)  # throttle globally\n",
    "\n",
    "    query = f\"{name} {club}\".replace(\" \", \"+\")\n",
    "    url = DUCKDUCKGO_SEARCH + query + \"&ia=web\"\n",
    "    match = None\n",
    "\n",
    "    try:\n",
    "        await tab.goto(url)\n",
    "        # await tab.wait_for_timeout(500)\n",
    "\n",
    "        links = await tab.locator(\"a\").all()\n",
    "        for link in links:\n",
    "            href = await link.get_attribute(\"href\")\n",
    "            if href and \"fbref.com/en/players/\" in href:\n",
    "                match = href\n",
    "                break\n",
    "\n",
    "        with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "            if match:\n",
    "                df.at[index, \"fbref_url\"] = match\n",
    "                log.write(f\"Found: {name} ({club}) ‚Üí {match}\\n\")\n",
    "            else:\n",
    "                log.write(f\"Not found: {name} ({club})\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"Error on {name} ({club}): {e}\\n\")\n",
    "\n",
    "\n",
    "# Scrape all players\n",
    "async def run_scraper():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        tab = await browser.new_page()\n",
    "\n",
    "        for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Scraping FBref links\"):\n",
    "            if pd.notnull(row[\"fbref_url\"]):\n",
    "                continue  # already scraped\n",
    "            else:\n",
    "                name = str(row[\"Name\"]).strip()\n",
    "                club = str(row.get(\"Club\", \"\")).strip()\n",
    "                await scrape_player(tab, i, name, club)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Done! CSV saved to {CSV_PATH} | Logs saved to {LOG_PATH}\")\n",
    "\n",
    "\n",
    "# Launch\n",
    "await run_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from playwright.async_api import TimeoutError\n",
    "from tqdm import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "DUCKDUCKGO_SEARCH = \"https://duckduckgo.com/?q=site%3Afbref.com+\"\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 500  # milliseconds\n",
    "\n",
    "# Load players from CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"fbref_url\" not in df.columns:\n",
    "    df[\"fbref_url\"] = None\n",
    "\n",
    "\n",
    "async def search_fbref_via_duckduckgo():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        for i, row in tqdm(df.iterrows(), total=len(df), desc=\"üîé Searching players\"):\n",
    "            if pd.notna(row[\"fbref_url\"]):\n",
    "                print(f\"‚è© Skipped: {row['Name']} (already found)\")\n",
    "                continue\n",
    "\n",
    "            name = str(row[\"Name\"]).strip()\n",
    "            query_url = DUCKDUCKGO_SEARCH + name.replace(\" \", \"+\") + \"&ia=web\"\n",
    "            match = None\n",
    "\n",
    "            for attempt in range(1, MAX_RETRIES + 1):\n",
    "                try:\n",
    "                    await page.goto(query_url, wait_until=\"domcontentloaded\")\n",
    "                    await page.wait_for_timeout(400)\n",
    "\n",
    "                    links = await page.locator(\"a\").all()\n",
    "                    for link in links:\n",
    "                        href = await link.get_attribute(\"href\")\n",
    "                        if href and \"fbref.com/en/players/\" in href:\n",
    "                            match = href\n",
    "                            break\n",
    "\n",
    "                    if match:\n",
    "                        break\n",
    "\n",
    "                except TimeoutError:\n",
    "                    print(f\"‚ö†Ô∏è Timeout on attempt {attempt} for {name}\")\n",
    "                    await page.wait_for_timeout(RETRY_DELAY)\n",
    "\n",
    "            if match:\n",
    "                df.at[i, \"fbref_url\"] = match\n",
    "                print(f\"Found: {name} ‚Üí {match}\")\n",
    "                df.to_csv(CSV_PATH, index=False)  # Save immediately\n",
    "            else:\n",
    "                print(f\"Not found after {MAX_RETRIES} attempts: {name}\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    print(f\"All done! Saved to {CSV_PATH}\")\n",
    "\n",
    "\n",
    "# Run the scraping function\n",
    "await search_fbref_via_duckduckgo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecbf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install asyncio nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5caa2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install playwright==1.39.0 nest_asyncio==1.5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2322309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "DUCKDUCKGO_SEARCH = \"https://duckduckgo.com/?q=site%3Afbref.com+\"\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 0.5  # In seconds\n",
    "CONCURRENT_WORKERS = 10\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)...\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)...\",\n",
    "]\n",
    "\n",
    "PROXY = {\n",
    "    \"server\": \"http://your.proxy.server:port\",\n",
    "    \"username\": \"proxy_user\",\n",
    "    \"password\": \"proxy_pass\",\n",
    "}\n",
    "\n",
    "# Load and shuffle players\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"fbref_url\" not in df.columns:\n",
    "    df[\"fbref_url\"] = None\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "async def process_player(context, i, row):\n",
    "    if pd.notna(row[\"fbref_url\"]):\n",
    "        return\n",
    "\n",
    "    name = str(row[\"Name\"]).strip()\n",
    "    query_url = DUCKDUCKGO_SEARCH + name.replace(\" \", \"+\") + \"&ia=web\"\n",
    "    match = None\n",
    "    page = await context.new_page()\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            await page.goto(query_url, wait_until=\"domcontentloaded\", timeout=10000)\n",
    "            links = await page.locator(\"a\").all()\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"fbref.com/en/players/\" in href:\n",
    "                    match = href\n",
    "                    break\n",
    "            if match:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt} failed for {name}: {e}\")\n",
    "            await asyncio.sleep(RETRY_DELAY)\n",
    "\n",
    "    await page.close()\n",
    "\n",
    "    if match:\n",
    "        df.at[i, \"fbref_url\"] = match\n",
    "        print(f\"{name}: {match}\")\n",
    "    else:\n",
    "        print(f\"{name}: Not found\")\n",
    "\n",
    "    await asyncio.sleep(random.uniform(1.5, 4.0))\n",
    "\n",
    "\n",
    "async def search_fbref_concurrent():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        sem = asyncio.Semaphore(CONCURRENT_WORKERS)\n",
    "\n",
    "        async def limited_worker(i, row):\n",
    "            async with sem:\n",
    "                agent = random.choice(USER_AGENTS)\n",
    "                context = await browser.new_context(user_agent=agent, proxy=PROXY)\n",
    "                try:\n",
    "                    await process_player(context, i, row)\n",
    "                finally:\n",
    "                    await context.close()\n",
    "\n",
    "        tasks = [limited_worker(i, row) for i, row in df.iterrows()]\n",
    "        for f in tqdm(\n",
    "            asyncio.as_completed(tasks), total=len(tasks), desc=\"üöÄ Scraping\"\n",
    "        ):\n",
    "            await f\n",
    "\n",
    "        await browser.close()\n",
    "        df.to_csv(CSV_PATH, index=False)\n",
    "        print(f\"Completed. Saved to {CSV_PATH}\")\n",
    "\n",
    "\n",
    "# Run the concurrent scraper\n",
    "asyncio.run(search_fbref_concurrent())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "import certifi\n",
    "\n",
    "ssl_context = ssl.create_default_context(cafile=certifi.where())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ebec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
