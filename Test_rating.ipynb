{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d811d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.34.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: bs4 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: playwright in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.39.0)\n",
      "Requirement already satisfied: urllib3~=2.5.0 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]~=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2025.7.9)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\l1160681\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: greenlet==3.0.0 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from playwright) (3.0.0)\n",
      "Requirement already satisfied: pyee==11.0.1 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from playwright) (11.0.1)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: pycparser in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium tqdm bs4 playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7746ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\l1160681\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\l1160681\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed14fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e071a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîó Constants\n",
    "BASE_URL = (\n",
    "    \"https://www.footballtransfers.com/en/values/players/most-valuable-players/{}\"\n",
    ")\n",
    "MAX_RETRIES = 8\n",
    "THREADS = 7\n",
    "MAX_PAGES = 1353\n",
    "TIMEOUT_SECONDS = 1\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "DATA_FILE = \"most_valuable_players_fast.csv\"\n",
    "SCRAPED_PAGES_LOG = \"scraped_pages.txt\"\n",
    "FAILED_PAGES_LOG = \"failed_pages.txt\"\n",
    "\n",
    "\n",
    "# üìÑ Page Logging Helpers\n",
    "def load_page_log(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return set(int(line.strip()) for line in f if line.strip().isdigit())\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "\n",
    "def log_page(file_path, page_number):\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"{page_number}\\n\")\n",
    "\n",
    "\n",
    "# üß≠ Setup WebDriver\n",
    "def create_driver():\n",
    "    options = Options()\n",
    "    options.binary_location = CHROMIUM_PATH\n",
    "    options.add_argument(\"--headless=chrome\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    prefs = {\n",
    "        \"profile.managed_default_content_settings.images\": 2,\n",
    "        \"profile.managed_default_content_settings.stylesheets\": 2,\n",
    "        \"profile.managed_default_content_settings.cookies\": 2,\n",
    "        \"profile.managed_default_content_settings.javascript\": 1,\n",
    "        \"profile.managed_default_content_settings.plugins\": 2,\n",
    "        \"profile.managed_default_content_settings.popups\": 2,\n",
    "        \"profile.managed_default_content_settings.geolocation\": 2,\n",
    "        \"profile.managed_default_content_settings.media_stream\": 2,\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "# üß™ Parse HTML Page\n",
    "def parse_html(page_source):\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    rows = soup.find_all(\"tr\")\n",
    "    data = []\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            name_tag = row.select_one(\"td.td-player a[title]\")\n",
    "            if not name_tag:\n",
    "                continue\n",
    "            name = name_tag.get_text(strip=True)\n",
    "            player_url = \"https://www.footballtransfers.com\" + name_tag[\"href\"]\n",
    "\n",
    "            age_tag = row.select_one(\"td.age\")\n",
    "            age = age_tag.get_text(strip=True) if age_tag else None\n",
    "\n",
    "            club_tag = row.select_one(\"td.td-player .sub-text a[title]\")\n",
    "            club = club_tag.get_text(strip=True) if club_tag else None\n",
    "            club_url = (\n",
    "                \"https://www.footballtransfers.com\" + club_tag[\"href\"]\n",
    "                if club_tag\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            position_tag = row.select_one(\"td.td-player span.sub-text\")\n",
    "            position = (\n",
    "                position_tag.get_text(strip=True).split(\"‚Ä¢\")[-1].strip()\n",
    "                if position_tag\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            nationality_tag = row.select_one(\"td.td-player figure img\")\n",
    "            nationality = nationality_tag[\"alt\"] if nationality_tag else None\n",
    "\n",
    "            skill_tag = row.select_one(\"div.table-skill__skill\")\n",
    "            skill = float(skill_tag.get_text(strip=True)) if skill_tag else None\n",
    "\n",
    "            potential_tag = row.select_one(\"div.table-skill__pot\")\n",
    "            potential = (\n",
    "                float(potential_tag.get_text(strip=True)) if potential_tag else None\n",
    "            )\n",
    "\n",
    "            value_tag = row.select_one(\"span.player-tag\")\n",
    "            value = (\n",
    "                value_tag.get_text(strip=True).replace(\"‚Ç¨\", \"\") if value_tag else None\n",
    "            )\n",
    "\n",
    "            if value and \"M\" in value:\n",
    "                market_value = float(value.replace(\"M\", \"\")) * 1e6\n",
    "            elif value and \"K\" in value:\n",
    "                market_value = float(value.replace(\"K\", \"\")) * 1e3\n",
    "            else:\n",
    "                market_value = None\n",
    "\n",
    "            data.append(\n",
    "                {\n",
    "                    \"Name\": name,\n",
    "                    \"Player URL\": player_url,\n",
    "                    \"Age\": age,\n",
    "                    \"Club\": club,\n",
    "                    \"Club URL\": club_url,\n",
    "                    \"Position\": position,\n",
    "                    \"Nationality\": nationality,\n",
    "                    \"Skill\": skill,\n",
    "                    \"Potential\": potential,\n",
    "                    \"Market Value (‚Ç¨)\": market_value,\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Parse error: {e}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# üöú Scrape Range\n",
    "def scrape_page_range(start_page, end_page, scraped_pages):\n",
    "    driver = create_driver()\n",
    "    wait = WebDriverWait(driver, TIMEOUT_SECONDS)\n",
    "\n",
    "    for page in tqdm(\n",
    "        range(start_page, end_page + 1),\n",
    "        desc=f\"Thread {start_page}-{end_page}\",\n",
    "        leave=False,\n",
    "    ):\n",
    "        if page in scraped_pages:\n",
    "            print(f\"‚è≠Ô∏è Skipping page {page} (already scraped)\")\n",
    "            continue\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                url = BASE_URL.format(page)\n",
    "                driver.get(url)\n",
    "                wait.until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (By.CSS_SELECTOR, \"td.td-player a[title]\")\n",
    "                    )\n",
    "                )\n",
    "                data = parse_html(driver.page_source)\n",
    "\n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df.to_csv(\n",
    "                        DATA_FILE,\n",
    "                        mode=\"a\",\n",
    "                        header=not os.path.exists(DATA_FILE),\n",
    "                        index=False,\n",
    "                    )\n",
    "                    log_page(SCRAPED_PAGES_LOG, page)\n",
    "                    print(f\"‚úÖ Saved page {page} with {len(data)} players\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è No data found on page {page}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"üîÅ Retry {attempt} failed on page {page}: {e}\")\n",
    "                time.sleep(2 * attempt)\n",
    "                if attempt == MAX_RETRIES:\n",
    "                    log_page(FAILED_PAGES_LOG, page)\n",
    "                    print(f\"‚ùå Failed page {page} after {MAX_RETRIES} retries\")\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# üöÄ Main Execution\n",
    "def main():\n",
    "    chunk_size = math.ceil(MAX_PAGES / THREADS)\n",
    "    ranges = [\n",
    "        (i, min(i + chunk_size - 1, MAX_PAGES))\n",
    "        for i in range(1, MAX_PAGES + 1, chunk_size)\n",
    "    ]\n",
    "    scraped_pages = load_page_log(SCRAPED_PAGES_LOG)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        futures = [\n",
    "            executor.submit(scrape_page_range, start, end, scraped_pages)\n",
    "            for start, end in ranges\n",
    "        ]\n",
    "        for future in tqdm(\n",
    "            as_completed(futures), total=len(futures), desc=\"üîÑ Scraping Progress\"\n",
    "        ):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Thread error: {e}\")\n",
    "\n",
    "    # üìä Summary Report\n",
    "    scraped = load_page_log(SCRAPED_PAGES_LOG)\n",
    "    failed = load_page_log(FAILED_PAGES_LOG)\n",
    "    all_attempted = scraped | failed\n",
    "    skipped = set(range(1, MAX_PAGES + 1)) - all_attempted\n",
    "\n",
    "    print(\"\\nüìä Summary:\")\n",
    "    print(f\"‚úÖ Scraped pages: {len(scraped)}\")\n",
    "    print(f\"‚ùå Failed pages: {len(failed)} (see '{FAILED_PAGES_LOG}')\")\n",
    "    print(f\"‚è≠Ô∏è Skipped pages: {len(skipped)} (not attempted yet)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "# Read file as raw text\n",
    "with open(\"most_valuable_players_fast.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    csv_data = f.read()\n",
    "\n",
    "# Replace doubled double-quotes (\"\"M, AM (R)\"\") with proper quoting (\"M, AM (R)\")\n",
    "csv_data = csv_data.replace('\"\"', '\"')\n",
    "\n",
    "# Load CSV from string safely\n",
    "df = pd.read_csv(StringIO(csv_data), quotechar='\"', skipinitialspace=True)\n",
    "\n",
    "# Convert numeric columns\n",
    "for col in [\"Age\", \"Rating\", \"Potential\", \"Value\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Show preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd7644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name                                        Player link  Age  \\\n",
      "0   Erling Haaland  https://www.footballtransfers.comhttps://www.f...   24   \n",
      "1     Lamine Yamal  https://www.footballtransfers.comhttps://www.f...   17   \n",
      "2    Kylian Mbapp√©  https://www.footballtransfers.comhttps://www.f...   26   \n",
      "3            Pedri  https://www.footballtransfers.comhttps://www.f...   22   \n",
      "4  Jude Bellingham  https://www.footballtransfers.comhttps://www.f...   22   \n",
      "\n",
      "          Team                                          Team Link  \\\n",
      "0     Man City  https://www.footballtransfers.comhttps://www.f...   \n",
      "1    Barcelona  https://www.footballtransfers.comhttps://www.f...   \n",
      "2  Real Madrid  https://www.footballtransfers.comhttps://www.f...   \n",
      "3    Barcelona  https://www.footballtransfers.comhttps://www.f...   \n",
      "4  Real Madrid  https://www.footballtransfers.comhttps://www.f...   \n",
      "\n",
      "        Positions Nationality  Rating  Potential        Value  \n",
      "0           F (C)      Norway    91.7       99.3  184200000.0  \n",
      "1       M, AM (R)       Spain    83.9      100.0  183200000.0  \n",
      "2           F (C)      France    89.7       93.8  153200000.0  \n",
      "3   M, DM, AM (C)       Spain    83.8       97.5  137900000.0  \n",
      "4  AM (C), M (CL)     England    83.1       97.7  137800000.0  \n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Your raw CSV string\n",
    "csv_data = \"\"\"Name,Player link,Age,Team,Team Link,Positions,Nationality,Rating,Potential,Value\n",
    "Erling Haaland,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/erling-braut-haaland,24,Man City,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/uk/man-city,F (C),Norway,91.7,99.3,184200000.0\n",
    "Lamine Yamal,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/lamine-yamal,17,Barcelona,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/es/barcelona,\"M, AM (R)\",Spain,83.9,100.0,183200000.0\n",
    "Kylian Mbapp√©,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/kylian-mbappe,26,Real Madrid,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/es/real-madrid,F (C),France,89.7,93.8,153200000.0\n",
    "Pedri,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/pedri-1,22,Barcelona,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/es/barcelona,\"M, DM, AM (C)\",Spain,83.8,97.5,137900000.0\n",
    "Jude Bellingham,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/jude-bellingham,22,Real Madrid,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/es/real-madrid,\"AM (C), M (CL)\",England,83.1,97.7,137800000.0\n",
    "Florian Wirtz,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/florian-wirtz,22,Liverpool,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/uk/liverpool,AM (C),Germany,86.3,98.8,135000000.0\n",
    "Pau Cubars√≠,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/pau-cubarsi,18,Barcelona,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/es/barcelona,D (C),Spain,71.7,100.0,130900000.0\n",
    "Jamal Musiala,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/players/jamal-musiala,22,Bayern,https://www.footballtransfers.comhttps://www.footballtransfers.com/en/teams/de/bayern,M (C),Germany,89.4,100.0,129300000.00000001\n",
    "\"\"\"\n",
    "\n",
    "# Parse the CSV string safely\n",
    "df = pd.read_csv(StringIO(csv_data), quotechar='\"')\n",
    "\n",
    "# Optional: convert numeric fields\n",
    "for col in [\"Age\", \"Rating\", \"Potential\", \"Value\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Preview clean data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"most_valuable_players_fast.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    csv_data = f.read()\n",
    "\n",
    "print(csv_data[:1000])  # Preview first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9074a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fixed_rows = []\n",
    "expected_columns = 10\n",
    "\n",
    "with open(\"most_valuable_players_fast.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "\n",
    "        # If entire line is wrapped in quotes, unwrap it first\n",
    "        if line.startswith('\"') and line.endswith('\"'):\n",
    "            line = line[1:-1]\n",
    "\n",
    "        # Fix escaped quotes like \"\"M, AM (R)\"\" to \"M, AM (R)\"\n",
    "        line = line.replace('\"\"', '\"')\n",
    "\n",
    "        # Parse line using csv.reader for proper quote handling\n",
    "        parsed = next(csv.reader([line], quotechar='\"', skipinitialspace=True))\n",
    "\n",
    "        # If the row looks good, keep it\n",
    "        if len(parsed) == expected_columns:\n",
    "            fixed_rows.append(parsed)\n",
    "        elif len(parsed) > expected_columns:\n",
    "            # Try merging overflow into Positions field\n",
    "            repaired = parsed[:5] + [\", \".join(parsed[5:-4])] + parsed[-4:]\n",
    "            if len(repaired) == expected_columns:\n",
    "                fixed_rows.append(repaired)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Could not fix row: {parsed}\")\n",
    "\n",
    "# Load into DataFrame\n",
    "columns = [\n",
    "    \"Name\",\n",
    "    \"Player URL\",\n",
    "    \"Age\",\n",
    "    \"Team\",\n",
    "    \"Team Link\",\n",
    "    \"Positions\",\n",
    "    \"Nationality\",\n",
    "    \"Rating\",\n",
    "    \"Potential\",\n",
    "    \"Value\",\n",
    "]\n",
    "df = pd.DataFrame(fixed_rows, columns=columns)\n",
    "\n",
    "# Convert numeric columns\n",
    "for col in [\"Age\", \"Rating\", \"Potential\", \"Value\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Preview cleaned data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(index=0).reset_index(drop=True)\n",
    "# Define the redundant prefix\n",
    "prefix = \"https://www.footballtransfers.com\"\n",
    "\n",
    "# Fix repeated prefix in all relevant URL columns\n",
    "for col in [\"Player URL\", \"Team Link\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].str.replace(f\"{prefix}{prefix}\", prefix, regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8746355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all_players_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce73c805",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Sort by 'Value' descending\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_sorted = \u001b[43mdf\u001b[49m.sort_values(by=\u001b[33m\"\u001b[39m\u001b[33mValue\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Preview top 10 most valuable players\u001b[39;00m\n\u001b[32m      5\u001b[39m df_sorted[[\u001b[33m\"\u001b[39m\u001b[33mName\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTeam\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mValue\u001b[39m\u001b[33m\"\u001b[39m]][:\u001b[32m50\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Sort by 'Value' descending\n",
    "df_sorted = df.sort_values(by=\"Value\", ascending=False)\n",
    "\n",
    "# Preview top 10 most valuable players\n",
    "df_sorted[[\"Name\", \"Team\", \"Value\"]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for French players\n",
    "french_players = df[df[\"Nationality\"] == \"France\"]\n",
    "\n",
    "# Optional: sort by Value descending\n",
    "french_players_sorted = french_players.sort_values(by=\"Rating\", ascending=False)\n",
    "\n",
    "# Show relevant columns\n",
    "french_players_sorted[[\"Name\", \"Team\", \"Value\", \"Rating\", \"Potential\"]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c43dd7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in c:\\users\\l1160681\\appdata\\local\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee84807",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m         browser.close()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mrun_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrun_scrape\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_scrape\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msync_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchromium\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadless\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\anaconda3\\Lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[39m, in \u001b[36mPlaywrightContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     45\u001b[39m             \u001b[38;5;28mself\u001b[39m._own_loop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._loop.is_running():\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[32m     48\u001b[39m \u001b[38;5;250m                \u001b[39m\u001b[33;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m             )\n\u001b[32m     52\u001b[39m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[32m     53\u001b[39m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[32m     54\u001b[39m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgreenlet_main\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mError\u001b[39m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "\n",
    "def run_scrape():\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "        page.goto(\n",
    "            \"https://fbref.com/en/players/7a2e46a8/all_comps/Alisson-Stats---All-Competitions\"\n",
    "        )\n",
    "        page.wait_for_selector(\"#stats_keeper_expanded\")\n",
    "        table_html = page.locator(\"#stats_keeper_expanded\").inner_html()\n",
    "        print(table_html)\n",
    "        browser.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cfb988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "\n",
    "\n",
    "async def fetch_table_and_save_csv():\n",
    "    url = \"https://fbref.com/en/players/7a2e46a8/all_comps/Alisson-Stats---All-Competitions\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # Wait for rows to ensure content is loaded\n",
    "        await page.wait_for_selector(\"#stats_keeper_expanded >> tbody >> tr\")\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"id\": \"stats_keeper_expanded\"})\n",
    "    if not table:\n",
    "        print(\"‚ö†Ô∏è Table not found.\")\n",
    "        return\n",
    "\n",
    "    # Use last <tr> in <thead> as the real header row\n",
    "    header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "\n",
    "    data = []\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        # Include potential <th> (for sticky first column like \"Season\")\n",
    "        ths = [th.get_text(strip=True) for th in tr.find_all(\"th\")]\n",
    "        tds = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "        row = ths + tds  # combine <th> and <td> values\n",
    "\n",
    "        if len(row) == len(headers):\n",
    "            data.append(row)\n",
    "        else:\n",
    "            print(\n",
    "                f\"‚ö†Ô∏è Row mismatch: expected {len(headers)} but got {len(row)} ‚Üí Skipped row:\",\n",
    "                row,\n",
    "            )\n",
    "\n",
    "    if not data:\n",
    "        print(\"‚ö†Ô∏è No valid data rows found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    df.to_csv(\"alisson_goalkeeping_stats.csv\", index=False)\n",
    "    print(\"‚úÖ Saved as alisson_goalkeeping_stats.csv\")\n",
    "\n",
    "\n",
    "# Run it\n",
    "asyncio.run(fetch_table_and_save_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "BASE_URL = \"https://fbref.com\"\n",
    "\n",
    "\n",
    "async def fetch_table_and_save_csv():\n",
    "    url = f\"{BASE_URL}/en/players/7a2e46a8/all_comps/Alisson-Stats---All-Competitions\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # Wait for actual data rows to be visible\n",
    "        await page.wait_for_selector(\"#stats_keeper_expanded >> tbody >> tr\")\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"id\": \"stats_keeper_expanded\"})\n",
    "    if not table:\n",
    "        print(\"‚ö†Ô∏è Table not found.\")\n",
    "        return\n",
    "\n",
    "    # Get accurate column names from last header row\n",
    "    header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "\n",
    "    data = []\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        row = []\n",
    "\n",
    "        # Handle sticky <th> cells (e.g. \"Season\") and add link if present\n",
    "        for th in tr.find_all(\"th\"):\n",
    "            text = th.get_text(strip=True)\n",
    "            link = th.find(\"a\")\n",
    "            if link and link.get(\"href\"):\n",
    "                text += f\" ({BASE_URL}{link.get('href')})\"\n",
    "            row.append(text)\n",
    "\n",
    "        # Handle <td> cells with potential links\n",
    "        for td in tr.find_all(\"td\"):\n",
    "            text = td.get_text(strip=True)\n",
    "            link = td.find(\"a\")\n",
    "            if link and link.get(\"href\"):\n",
    "                text += f\" ({BASE_URL}{link.get('href')})\"\n",
    "            row.append(text)\n",
    "\n",
    "        if len(row) == len(headers):\n",
    "            data.append(row)\n",
    "        else:\n",
    "            print(\n",
    "                f\"‚ö†Ô∏è Row mismatch: expected {len(headers)} but got {len(row)} ‚Üí Skipping.\"\n",
    "            )\n",
    "\n",
    "    if not data:\n",
    "        print(\"‚ö†Ô∏è No valid data rows found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    df.to_csv(\"alisson_goalkeeping_stats_upgraded.csv\", index=False)\n",
    "    print(\"‚úÖ Saved as alisson_goalkeeping_stats_upgraded.csv with links included\")\n",
    "\n",
    "\n",
    "# Execute the async function\n",
    "asyncio.run(fetch_table_and_save_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# Patch event loop (required for Jupyter or interactive mode)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://fbref.com\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "\n",
    "\n",
    "async def fetch_national_team_stats():\n",
    "    url = f\"{BASE_URL}/en/players/7a2e46a8/all_comps/Alisson-Stats---All-Competitions\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # OPTIONAL: Wait for a key element (e.g. \"Matches\" tab) to ensure page loads\n",
    "        await asyncio.sleep(4)  # allow time for JS-rendered tables\n",
    "\n",
    "        # Get page HTML\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Locate the correct national team keeper table by ID\n",
    "    table = soup.find(\"table\", {\"id\": \"stats_keeper_nat_tm\"})\n",
    "    if not table:\n",
    "        print(\"‚ö†Ô∏è National Team goalkeeping table not found.\")\n",
    "        return\n",
    "\n",
    "    # Extract column headers\n",
    "    header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "\n",
    "    # Extract table rows\n",
    "    data = []\n",
    "    for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "        row = []\n",
    "\n",
    "        # <th> cells (e.g. season) + optional link\n",
    "        for th in tr.find_all(\"th\"):\n",
    "            text = th.get_text(strip=True)\n",
    "            link = th.find(\"a\")\n",
    "            if link and link.get(\"href\"):\n",
    "                text += f\" ({BASE_URL}{link.get('href')})\"\n",
    "            row.append(text)\n",
    "\n",
    "        # <td> cells + optional link\n",
    "        for td in tr.find_all(\"td\"):\n",
    "            text = td.get_text(strip=True)\n",
    "            link = td.find(\"a\")\n",
    "            if link and link.get(\"href\"):\n",
    "                text += f\" ({BASE_URL}{link.get('href')})\"\n",
    "            row.append(text)\n",
    "\n",
    "        # Verify row matches header length\n",
    "        if len(row) == len(headers):\n",
    "            data.append(row)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipped row ({len(row)} vs {len(headers)} columns):\", row)\n",
    "\n",
    "    # Export to CSV\n",
    "    if data:\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        df.to_csv(\"alisson_national_team_stats.csv\", index=False)\n",
    "        print(\"‚úÖ Saved as alisson_national_team_stats.csv with embedded links\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid rows found.\")\n",
    "\n",
    "\n",
    "# Run the function in Notebook\n",
    "await fetch_national_team_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Settings\n",
    "BASE_URL = \"https://fbref.com\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "SAVE_FOLDER = \"alisson_all_fbref_tables\"\n",
    "\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "async def scrape_all_fbref_tables():\n",
    "    url = f\"{BASE_URL}/en/players/7a2e46a8/all_comps/Alisson-Stats---All-Competitions\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # Trigger all preset buttons to reveal toggled tables\n",
    "        await page.evaluate(\"\"\"\n",
    "            document.querySelectorAll('a.sr_preset').forEach(el => el.click());\n",
    "        \"\"\")\n",
    "        await asyncio.sleep(5)  # Let the page render updated sections\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tables = soup.find_all(\"table\")\n",
    "\n",
    "    print(f\"üîç Found {len(tables)} tables on page\")\n",
    "\n",
    "    for table in tables:\n",
    "        table_id = table.get(\"id\", None)\n",
    "        if not table_id:\n",
    "            continue  # skip tables without ID\n",
    "\n",
    "        print(f\"üìä Processing table: {table_id}\")\n",
    "\n",
    "        # Extract headers\n",
    "        header_row = (\n",
    "            table.find(\"thead\").find_all(\"tr\")[-1] if table.find(\"thead\") else None\n",
    "        )\n",
    "        headers = (\n",
    "            [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "            if header_row\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        rows = []\n",
    "        for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            row = []\n",
    "            for cell in tr.find_all([\"th\", \"td\"]):\n",
    "                text = cell.get_text(strip=True)\n",
    "                link = cell.find(\"a\")\n",
    "                if link and link.get(\"href\"):\n",
    "                    text += f\" ({BASE_URL}{link.get('href')})\"\n",
    "                row.append(text)\n",
    "            if len(row) == len(headers):\n",
    "                rows.append(row)\n",
    "\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows, columns=headers)\n",
    "            output_path = os.path.join(SAVE_FOLDER, f\"{table_id}.csv\")\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"‚úÖ Saved: {output_path}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipped empty table: {table_id}\")\n",
    "\n",
    "\n",
    "# Run the function\n",
    "await scrape_all_fbref_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual file path or name\n",
    "df = pd.read_csv(\"all_players_ratings.csv\")\n",
    "\n",
    "# Optional: Preview the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "CSV_PATH = \"all_players_ratings.csv\"  # üîÅ Update with your actual file\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "SEARCH_BASE = \"https://fbref.com/en/search/search.fcgi?search=\"\n",
    "\n",
    "# Load player list\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = None  # New column to store FBref profile link\n",
    "\n",
    "\n",
    "async def search_fbref_for_players():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            name = str(row[\"Name\"]).strip()\n",
    "            search_url = SEARCH_BASE + name.replace(\" \", \"+\")\n",
    "            await page.goto(search_url)\n",
    "            await page.wait_for_timeout(1500)  # ‚è≥ small delay to allow page content\n",
    "\n",
    "            # Look for first profile link\n",
    "            links = await page.locator(\"a\").all()\n",
    "            match = None\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"/en/players/\" in href and \"all_comps\" in href:\n",
    "                    match = \"https://fbref.com\" + href\n",
    "                    break\n",
    "\n",
    "            if match:\n",
    "                df.at[i, \"fbref_url\"] = match\n",
    "                print(f\"‚úÖ Found: {name} ‚Üí {match}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Not found: {name}\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # Export enriched data\n",
    "    output_path = \"players_with_fbref_links.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ All done! Saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Run the function\n",
    "await search_fbref_for_players()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "SEARCH_BASE = \"https://fbref.com/en/search/search.fcgi?search=\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = None\n",
    "\n",
    "\n",
    "async def search_fbref_for_players():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            name = str(row[\"Name\"]).strip()\n",
    "            search_url = SEARCH_BASE + name.replace(\" \", \"+\")\n",
    "            await page.goto(search_url)\n",
    "            await page.wait_for_timeout(1500)\n",
    "\n",
    "            links = await page.locator(\"a\").all()\n",
    "            match_url = None\n",
    "\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"/en/players/\" in href:\n",
    "                    try:\n",
    "                        after_players = href.split(\"/players/\")[1]\n",
    "                        player_id = after_players.split(\"/\")[0]\n",
    "                        name_slug = name.replace(\" \", \"-\") + \"-Stats---All-Competitions\"\n",
    "                        match_url = f\"https://fbref.com/en/players/{player_id}/all_comps/{name_slug}\"\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "            if match_url:\n",
    "                df.at[i, \"fbref_url\"] = match_url\n",
    "                print(f\"‚úÖ {name} ‚Üí {match_url}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Not found: {name}\")\n",
    "\n",
    "        await browser.close()\n",
    "        df.to_csv(\"players_with_fbref_links.csv\", index=False)\n",
    "        print(\"\\nüìÅ Saved as players_with_fbref_links.csv\")\n",
    "\n",
    "\n",
    "await search_fbref_for_players()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bfda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# üîß Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"  # Update with your actual file path\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "DUCKDUCKGO_SEARCH = \"https://duckduckgo.com/?q=site%3Afbref.com+\"\n",
    "\n",
    "# üß† Load players from CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = None  # Add column for results\n",
    "\n",
    "\n",
    "async def search_fbref_via_duckduckgo():\n",
    "    async with async_playwright() as p:\n",
    "        # Launch browser\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        # Loop through players\n",
    "        for i, row in df.iterrows():\n",
    "            name = str(row[\"Name\"]).strip()\n",
    "            query_url = DUCKDUCKGO_SEARCH + name.replace(\" \", \"+\")\n",
    "            await page.goto(query_url)\n",
    "            await page.wait_for_timeout(2000)  # Wait for search results to load\n",
    "\n",
    "            match = None\n",
    "            links = await page.locator(\"a\").all()\n",
    "\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"fbref.com/en/players/\" in href:\n",
    "                    match = href\n",
    "                    break\n",
    "\n",
    "            if match:\n",
    "                df.at[i, \"fbref_url\"] = match\n",
    "                print(f\"‚úÖ Found: {name} ‚Üí {match}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Not found: {name}\")\n",
    "\n",
    "        # Clean up\n",
    "        await browser.close()\n",
    "\n",
    "    # üìù Save results\n",
    "    output_path = \"players_with_fbref_links_duckduckgo.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ All done! Saved to {output_path}\")\n",
    "\n",
    "\n",
    "# üöÄ Run the scraping function\n",
    "await search_fbref_via_duckduckgo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "DUCKDUCKGO_SEARCH = \"https://duckduckgo.com/?q=site%3Afbref.com+\"\n",
    "LOG_PATH = \"scraping_log.txt\"\n",
    "NUM_TABS = 5\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = df.get(\"fbref_url\", None)\n",
    "\n",
    "with open(LOG_PATH, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(f\"Scraping started at {datetime.now()}\\n\\n\")\n",
    "\n",
    "\n",
    "# üîÑ Scraping logic with retries\n",
    "async def scrape_player(tab, index, name):\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            query_url = DUCKDUCKGO_SEARCH + name.replace(\" \", \"+\")\n",
    "            await tab.goto(query_url)\n",
    "            await tab.wait_for_timeout(1500)\n",
    "\n",
    "            links = await tab.locator(\"a\").all()\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"fbref.com/en/players/\" in href:\n",
    "                    df.at[index, \"fbref_url\"] = href\n",
    "                    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                        log.write(f\"‚úÖ Found (Attempt {attempt}): {name} ‚Üí {href}\\n\")\n",
    "                    return\n",
    "            # If we get here, no match found\n",
    "        except Exception as e:\n",
    "            with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"‚ö†Ô∏è Error on {name} (Attempt {attempt}): {e}\\n\")\n",
    "\n",
    "    # Final failure\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"‚ùå Not found after {MAX_RETRIES} attempts: {name}\\n\")\n",
    "\n",
    "\n",
    "async def scrape_batch(tab, batch):\n",
    "    for index, name in tqdm(\n",
    "        batch, desc=f\"Tab scraping {len(batch)} players\", leave=False\n",
    "    ):\n",
    "        await scrape_player(tab, index, name)\n",
    "\n",
    "\n",
    "async def search_fbref_multi_tab():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        tabs = [await browser.new_page() for _ in range(NUM_TABS)]\n",
    "\n",
    "        all_players = [(i, str(row[\"Name\"]).strip()) for i, row in df.iterrows()]\n",
    "        batch_size = math.ceil(len(all_players) / NUM_TABS)\n",
    "        batches = [\n",
    "            all_players[i : i + batch_size]\n",
    "            for i in range(0, len(all_players), batch_size)\n",
    "        ]\n",
    "\n",
    "        await asyncio.gather(\n",
    "            *[scrape_batch(tab, batch) for tab, batch in zip(tabs, batches)]\n",
    "        )\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(\"\\n‚úÖ Scraping finished with retries and progress bar. CSV and log updated.\")\n",
    "\n",
    "\n",
    "await search_fbref_multi_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ‚úèÔ∏è Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "FBREF_SEARCH = \"https://fbref.com/en/search/search.fcgi?search=\"\n",
    "LOG_PATH = \"scraping_log.txt\"\n",
    "NUM_TABS = 1\n",
    "MAX_RETRIES = 3\n",
    "REQUEST_THROTTLE_SEC = 2\n",
    "STARTUP_DELAY_SEC = 0\n",
    "\n",
    "# üìÑ Load CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = df.get(\"fbref_url\", None)\n",
    "\n",
    "# üìù Init log file\n",
    "with open(LOG_PATH, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(f\"Scraping initialized at {datetime.now()}\\n\\n\")\n",
    "\n",
    "\n",
    "# üîÑ Player scraping logic\n",
    "async def scrape_player(tab, index, name):\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            search_url = FBREF_SEARCH + name.replace(\" \", \"+\")\n",
    "            await tab.goto(search_url)\n",
    "            await tab.wait_for_timeout(1500)\n",
    "            await asyncio.sleep(REQUEST_THROTTLE_SEC)\n",
    "\n",
    "            match = None\n",
    "            links = await tab.locator(\"a\").all()\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"/en/players/\" in href:\n",
    "                    match = \"https://fbref.com\" + href\n",
    "                    break\n",
    "\n",
    "            if not match:\n",
    "                table_links = await tab.locator(\"table a\").all()\n",
    "                for link in table_links:\n",
    "                    href = await link.get_attribute(\"href\")\n",
    "                    if href and \"/en/players/\" in href:\n",
    "                        match = \"https://fbref.com\" + href\n",
    "                        break\n",
    "\n",
    "            with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                if match:\n",
    "                    df.at[index, \"fbref_url\"] = match\n",
    "                    log.write(f\"‚úÖ Found (Attempt {attempt}): {name} ‚Üí {match}\\n\")\n",
    "                    return\n",
    "                else:\n",
    "                    log.write(f\"üîç No match on attempt {attempt}: {name}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"‚ö†Ô∏è Error on {name} (Attempt {attempt}): {e}\\n\")\n",
    "\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"‚ùå Not found after {MAX_RETRIES} attempts: {name}\\n\")\n",
    "\n",
    "\n",
    "# üßπ Batch logic\n",
    "async def scrape_batch(tab, batch):\n",
    "    for index, name in tqdm(\n",
    "        batch, desc=f\"üß™ Tab scraping {len(batch)} players\", leave=False\n",
    "    ):\n",
    "        await scrape_player(tab, index, name)\n",
    "\n",
    "\n",
    "# üöÄ Orchestration\n",
    "async def search_fbref_multi_tab():\n",
    "    print(\"‚è≥ Waiting 1 hour before scraping...\")\n",
    "    await asyncio.sleep(STARTUP_DELAY_SEC)\n",
    "    print(\"‚úÖ Starting scraping...\")\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        tabs = [await browser.new_page() for _ in range(NUM_TABS)]\n",
    "\n",
    "        all_players = [(i, str(row[\"Name\"]).strip()) for i, row in df.iterrows()]\n",
    "        batch_size = math.ceil(len(all_players) / NUM_TABS)\n",
    "        batches = [\n",
    "            all_players[i : i + batch_size]\n",
    "            for i in range(0, len(all_players), batch_size)\n",
    "        ]\n",
    "\n",
    "        await asyncio.gather(\n",
    "            *[scrape_batch(tab, batch) for tab, batch in zip(tabs, batches)]\n",
    "        )\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"\\nüìÅ Scraping finished. Updated CSV and log saved to {LOG_PATH}\")\n",
    "\n",
    "\n",
    "# üß® Run it\n",
    "await search_fbref_multi_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ‚úèÔ∏è Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "GOOGLE_SEARCH = \"https://www.google.com/search?q=site%3Afbref.com+\"\n",
    "LOG_PATH = \"scraping_log.txt\"\n",
    "NUM_TABS = 1\n",
    "MAX_RETRIES = 3\n",
    "REQUESTS_PER_MIN = 30\n",
    "STARTUP_DELAY_SEC = 0\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"fbref_url\"] = df.get(\"fbref_url\", None)\n",
    "\n",
    "# üìù Initialize log file\n",
    "with open(LOG_PATH, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(f\"Scraping initialized at {datetime.now()}\\n\\n\")\n",
    "\n",
    "# üîí Rate limiter across tabs\n",
    "rate_lock = asyncio.Lock()\n",
    "RATE_INTERVAL = 60 / REQUESTS_PER_MIN  # seconds between each request\n",
    "\n",
    "\n",
    "# üîÑ Scraping logic with shared global throttle\n",
    "async def scrape_player(tab, index, name, club):\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            async with rate_lock:\n",
    "                await asyncio.sleep(RATE_INTERVAL)  # enforce request interval globally\n",
    "\n",
    "            query = f\"{name} {club}\".replace(\" \", \"+\")\n",
    "            search_url = GOOGLE_SEARCH + query\n",
    "            await tab.goto(search_url)\n",
    "            await tab.wait_for_timeout(2000)\n",
    "\n",
    "            match = None\n",
    "            links = await tab.locator(\"a\").all()\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"fbref.com/en/players/\" in href:\n",
    "                    match = href\n",
    "                    break\n",
    "\n",
    "            with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                if match:\n",
    "                    df.at[index, \"fbref_url\"] = match\n",
    "                    log.write(\n",
    "                        f\"‚úÖ Found (Attempt {attempt}): {name} ({club}) ‚Üí {match}\\n\"\n",
    "                    )\n",
    "                    return\n",
    "                else:\n",
    "                    log.write(f\"üîç No match on attempt {attempt}: {name} ({club})\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "                log.write(f\"‚ö†Ô∏è Error on {name} ({club}) (Attempt {attempt}): {e}\\n\")\n",
    "\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(f\"‚ùå Not found after {MAX_RETRIES} attempts: {name} ({club})\\n\")\n",
    "\n",
    "\n",
    "# üßπ Batch logic\n",
    "async def scrape_batch(tab, batch):\n",
    "    for index, name, club in tqdm(\n",
    "        batch, desc=f\"üß™ Scraping {len(batch)} players\", leave=False\n",
    "    ):\n",
    "        await scrape_player(tab, index, name, club)\n",
    "\n",
    "\n",
    "# üöÄ Orchestration\n",
    "async def search_fbref_multi_tab():\n",
    "    if STARTUP_DELAY_SEC:\n",
    "        print(f\"‚è≥ Waiting {STARTUP_DELAY_SEC} seconds before scraping...\")\n",
    "        await asyncio.sleep(STARTUP_DELAY_SEC)\n",
    "    print(\"‚úÖ Starting scraping...\")\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        tabs = [await browser.new_page() for _ in range(NUM_TABS)]\n",
    "\n",
    "        all_players = [\n",
    "            (i, str(row[\"Name\"]).strip(), str(row.get(\"Club\", \"\")).strip())\n",
    "            for i, row in df.iterrows()\n",
    "        ]\n",
    "        batch_size = math.ceil(len(all_players) / NUM_TABS)\n",
    "        batches = [\n",
    "            all_players[i : i + batch_size]\n",
    "            for i in range(0, len(all_players), batch_size)\n",
    "        ]\n",
    "\n",
    "        await asyncio.gather(\n",
    "            *[scrape_batch(tab, batch) for tab, batch in zip(tabs, batches)]\n",
    "        )\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"\\nüìÅ Done! Updated CSV and saved logs to {LOG_PATH}\")\n",
    "\n",
    "\n",
    "# üß® Launch\n",
    "await search_fbref_multi_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99659121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ‚úèÔ∏è Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "DUCKDUCKGO_SEARCH = \"https://duckduckgo.com/?q=site%3Afbref.com+\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "LOG_PATH = \"scraping_log.txt\"\n",
    "RATE_LIMIT = 60 * 20  # requests per minute\n",
    "RATE_INTERVAL = 60 / RATE_LIMIT  # seconds between requests\n",
    "\n",
    "# üìÑ Load CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"fbref_url\" not in df.columns:\n",
    "    df[\"fbref_url\"] = None\n",
    "\n",
    "# üìù Init log\n",
    "with open(LOG_PATH, \"w\", encoding=\"utf-8\") as log:\n",
    "    log.write(f\"Scraping started at {datetime.now()}\\n\\n\")\n",
    "\n",
    "\n",
    "# üîÑ Single player search\n",
    "async def scrape_player(tab, index, name, club):\n",
    "    await asyncio.sleep(RATE_INTERVAL)  # throttle globally\n",
    "\n",
    "    query = f\"{name} {club}\".replace(\" \", \"+\")\n",
    "    url = DUCKDUCKGO_SEARCH + query + \"&ia=web\"\n",
    "    match = None\n",
    "\n",
    "    try:\n",
    "        await tab.goto(url)\n",
    "        # await tab.wait_for_timeout(500)\n",
    "\n",
    "        links = await tab.locator(\"a\").all()\n",
    "        for link in links:\n",
    "            href = await link.get_attribute(\"href\")\n",
    "            if href and \"fbref.com/en/players/\" in href:\n",
    "                match = href\n",
    "                break\n",
    "\n",
    "        with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "            if match:\n",
    "                df.at[index, \"fbref_url\"] = match\n",
    "                log.write(f\"‚úÖ Found: {name} ({club}) ‚Üí {match}\\n\")\n",
    "            else:\n",
    "                log.write(f\"‚ùå Not found: {name} ({club})\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"‚ö†Ô∏è Error on {name} ({club}): {e}\\n\")\n",
    "\n",
    "\n",
    "# üöÄ Scrape all players\n",
    "async def run_scraper():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        tab = await browser.new_page()\n",
    "\n",
    "        for i, row in tqdm(\n",
    "            df.iterrows(), total=len(df), desc=\"üîé Scraping FBref links\"\n",
    "        ):\n",
    "            if pd.notnull(row[\"fbref_url\"]):\n",
    "                continue  # already scraped\n",
    "            else:\n",
    "                name = str(row[\"Name\"]).strip()\n",
    "                club = str(row.get(\"Club\", \"\")).strip()\n",
    "                await scrape_player(tab, i, name, club)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"\\nüìÅ Done! CSV saved to {CSV_PATH} | Logs saved to {LOG_PATH}\")\n",
    "\n",
    "\n",
    "# üß® Launch\n",
    "await run_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from playwright.async_api import TimeoutError\n",
    "from tqdm import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# üîß Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "DUCKDUCKGO_SEARCH = \"https://duckduckgo.com/?q=site%3Afbref.com+\"\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 500  # milliseconds\n",
    "\n",
    "# üß† Load players from CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"fbref_url\" not in df.columns:\n",
    "    df[\"fbref_url\"] = None\n",
    "\n",
    "\n",
    "async def search_fbref_via_duckduckgo():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=False)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        for i, row in tqdm(df.iterrows(), total=len(df), desc=\"üîé Searching players\"):\n",
    "            if pd.notna(row[\"fbref_url\"]):\n",
    "                print(f\"‚è© Skipped: {row['Name']} (already found)\")\n",
    "                continue\n",
    "\n",
    "            name = str(row[\"Name\"]).strip()\n",
    "            query_url = DUCKDUCKGO_SEARCH + name.replace(\" \", \"+\") + \"&ia=web\"\n",
    "            match = None\n",
    "\n",
    "            for attempt in range(1, MAX_RETRIES + 1):\n",
    "                try:\n",
    "                    await page.goto(query_url, wait_until=\"domcontentloaded\")\n",
    "                    await page.wait_for_timeout(400)\n",
    "\n",
    "                    links = await page.locator(\"a\").all()\n",
    "                    for link in links:\n",
    "                        href = await link.get_attribute(\"href\")\n",
    "                        if href and \"fbref.com/en/players/\" in href:\n",
    "                            match = href\n",
    "                            break\n",
    "\n",
    "                    if match:\n",
    "                        break\n",
    "\n",
    "                except TimeoutError:\n",
    "                    print(f\"‚ö†Ô∏è Timeout on attempt {attempt} for {name}\")\n",
    "                    await page.wait_for_timeout(RETRY_DELAY)\n",
    "\n",
    "            if match:\n",
    "                df.at[i, \"fbref_url\"] = match\n",
    "                print(f\"‚úÖ Found: {name} ‚Üí {match}\")\n",
    "                df.to_csv(CSV_PATH, index=False)  # Save immediately\n",
    "            else:\n",
    "                print(f\"‚ùå Not found after {MAX_RETRIES} attempts: {name}\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    print(f\"\\n‚úÖ All done! Saved to {CSV_PATH}\")\n",
    "\n",
    "\n",
    "# üöÄ Run the scraping function\n",
    "await search_fbref_via_duckduckgo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aecbf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asyncio in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install asyncio nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5caa2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright==1.39.0 in c:\\users\\l1160681\\appdata\\local\\anaconda3\\lib\\site-packages (1.39.0)\n",
      "Requirement already satisfied: nest_asyncio==1.5.8 in c:\\users\\l1160681\\appdata\\local\\anaconda3\\lib\\site-packages (1.5.8)\n",
      "Requirement already satisfied: greenlet==3.0.0 in c:\\users\\l1160681\\appdata\\local\\anaconda3\\lib\\site-packages (from playwright==1.39.0) (3.0.0)\n",
      "Requirement already satisfied: pyee==11.0.1 in c:\\users\\l1160681\\appdata\\local\\anaconda3\\lib\\site-packages (from playwright==1.39.0) (11.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\l1160681\\appdata\\local\\anaconda3\\lib\\site-packages (from pyee==11.0.1->playwright==1.39.0) (4.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install playwright==1.39.0 nest_asyncio==1.5.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2322309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01a61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f0bbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Completed. Saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCSV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# üöÄ Run the concurrent scraper\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_fbref_concurrent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nest_asyncio.py:31\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     29\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nest_asyncio.py:99\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     98\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36msearch_fbref_concurrent\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch_fbref_concurrent\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m async_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[32m     76\u001b[39m         browser = \u001b[38;5;28;01mawait\u001b[39;00m p.chromium.launch(executable_path=CHROMIUM_PATH, headless=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     77\u001b[39m         sem = asyncio.Semaphore(CONCURRENT_WORKERS)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:47\u001b[39m, in \u001b[36mPlaywrightContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future.done():\n\u001b[32m     46\u001b[39m     playwright_future.cancel()\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m playwright = AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     48\u001b[39m playwright.stop = \u001b[38;5;28mself\u001b[39m.\u001b[34m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\playwright\\_impl\\_transport.py:123\u001b[39m, in \u001b[36mPipeTransport.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    120\u001b[39m         startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n\u001b[32m    121\u001b[39m         startupinfo.wShowWindow = subprocess.SW_HIDE\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[38;5;28mself\u001b[39m._proc = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m    124\u001b[39m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._driver_executable),\n\u001b[32m    125\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrun-driver\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    126\u001b[39m         stdin=asyncio.subprocess.PIPE,\n\u001b[32m    127\u001b[39m         stdout=asyncio.subprocess.PIPE,\n\u001b[32m    128\u001b[39m         stderr=_get_stderr_fileno(),\n\u001b[32m    129\u001b[39m         limit=\u001b[32m32768\u001b[39m,\n\u001b[32m    130\u001b[39m         env=env,\n\u001b[32m    131\u001b[39m         startupinfo=startupinfo,\n\u001b[32m    132\u001b[39m     )\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_error_future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\subprocess.py:224\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    221\u001b[39m loop = events.get_running_loop()\n\u001b[32m    222\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    223\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    225\u001b[39m     protocol_factory,\n\u001b[32m    226\u001b[39m     program, *args,\n\u001b[32m    227\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    228\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:1756\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1754\u001b[39m     debug_log = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1755\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1756\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1757\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1758\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1760\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, debug_log, transport)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\L1160681\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:528\u001b[39m, in \u001b[36mBaseEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[32m    525\u001b[39m                                      stdin, stdout, stderr, bufsize,\n\u001b[32m    526\u001b[39m                                      extra=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    527\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "# üõ† Patch the loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# üîß Configuration\n",
    "CSV_PATH = \"all_players_ratings.csv\"\n",
    "CHROMIUM_PATH = (\n",
    "    r\"C:/Users/L1160681/playwright-browsers/chromium-win64/chrome-win/chrome.exe\"\n",
    ")\n",
    "DUCKDUCKGO_SEARCH = \"https://duckduckgo.com/?q=site%3Afbref.com+\"\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 0.5  # In seconds\n",
    "CONCURRENT_WORKERS = 10\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)...\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64)...\",\n",
    "]\n",
    "\n",
    "PROXY = {\n",
    "    \"server\": \"http://your.proxy.server:port\",\n",
    "    \"username\": \"proxy_user\",\n",
    "    \"password\": \"proxy_pass\",\n",
    "}\n",
    "\n",
    "# üì¶ Load and shuffle players\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"fbref_url\" not in df.columns:\n",
    "    df[\"fbref_url\"] = None\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "async def process_player(context, i, row):\n",
    "    if pd.notna(row[\"fbref_url\"]):\n",
    "        return\n",
    "\n",
    "    name = str(row[\"Name\"]).strip()\n",
    "    query_url = DUCKDUCKGO_SEARCH + name.replace(\" \", \"+\") + \"&ia=web\"\n",
    "    match = None\n",
    "    page = await context.new_page()\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            await page.goto(query_url, wait_until=\"domcontentloaded\", timeout=10000)\n",
    "            links = await page.locator(\"a\").all()\n",
    "            for link in links:\n",
    "                href = await link.get_attribute(\"href\")\n",
    "                if href and \"fbref.com/en/players/\" in href:\n",
    "                    match = href\n",
    "                    break\n",
    "            if match:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt} failed for {name}: {e}\")\n",
    "            await asyncio.sleep(RETRY_DELAY)\n",
    "\n",
    "    await page.close()\n",
    "\n",
    "    if match:\n",
    "        df.at[i, \"fbref_url\"] = match\n",
    "        print(f\"‚úÖ {name}: {match}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: Not found\")\n",
    "\n",
    "    await asyncio.sleep(random.uniform(1.5, 4.0))\n",
    "\n",
    "\n",
    "async def search_fbref_concurrent():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(executable_path=CHROMIUM_PATH, headless=True)\n",
    "        sem = asyncio.Semaphore(CONCURRENT_WORKERS)\n",
    "\n",
    "        async def limited_worker(i, row):\n",
    "            async with sem:\n",
    "                agent = random.choice(USER_AGENTS)\n",
    "                context = await browser.new_context(user_agent=agent, proxy=PROXY)\n",
    "                try:\n",
    "                    await process_player(context, i, row)\n",
    "                finally:\n",
    "                    await context.close()\n",
    "\n",
    "        tasks = [limited_worker(i, row) for i, row in df.iterrows()]\n",
    "        for f in tqdm(\n",
    "            asyncio.as_completed(tasks), total=len(tasks), desc=\"üöÄ Scraping\"\n",
    "        ):\n",
    "            await f\n",
    "\n",
    "        await browser.close()\n",
    "        df.to_csv(CSV_PATH, index=False)\n",
    "        print(f\"\\n‚úÖ Completed. Saved to {CSV_PATH}\")\n",
    "\n",
    "\n",
    "# üöÄ Run the concurrent scraper\n",
    "asyncio.run(search_fbref_concurrent())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592e86ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi in c:\\users\\l1160681\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "import certifi\n",
    "\n",
    "ssl_context = ssl.create_default_context(cafile=certifi.where())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ebec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
